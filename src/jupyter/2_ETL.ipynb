{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923fcf31-fdb9-43a1-8e30-7b0037220e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b397a9e-006d-4714-801c-e9d2bedc52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook uses Spark to load an perform data transformations and combines interesting sections from these notebooks : \n",
    "#\n",
    "# https://www.kaggle.com/mohaiminul101/car-price-prediction\n",
    "# https://www.kaggle.com/aishu2218/do-you-wanna-predict-price-of-car-you-wanna-buy\n",
    "# https://www.kaggle.com/udit1907/linear-advanced-regression-guided-car-purchase\n",
    "#  \n",
    "# These notebooks use 'Pandas' and 'scikit-learn'. This notebook here uses Spark to do the ETL and \n",
    "# then converts the Spark dataframe to a 'Pandas' dataframe (at the last minute) to do the visualization.\n",
    "# Note that we are using regular 'Pandas' and not the newly available 'pyspark.pandas' in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b08a0-6afa-4e48-87a0-c6c34a39989b",
   "metadata": {},
   "source": [
    "# Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeac0e3-2cc2-4714-b986-8536a153c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simulated cluster environment is configured with : \n",
    "#   - 3 workers\n",
    "#   - With 3GB of memory for each worker (Total memory is 9GB)\n",
    "#   - Each worker has 2 cores (total cores is 6)\n",
    "\n",
    "# Start up Spark session. Let's be greedy and ask for all available resources (We'll be explicit).\n",
    "\n",
    "# We could not include the 'spark.executor.cores', 'spark.cores.max', 'spark.executor.memory' configurations. \n",
    "# Spark would give us all available resources by default (if fair sharing between jobs is not configured). \n",
    "\n",
    "# Request : \n",
    "#   - A maximum of 6 cores with \n",
    "#   - 2 cores per executor\n",
    "#   - 3 GB of memory per executor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab92e4-714a-4533-ba94-3a77690177af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .master(\"spark://spark-master:7077\")\\\n",
    "            .appName(\"etl_jupyter\")\\\n",
    "            .config(\"spark.executor.cores\", \"2\")\\\n",
    "            .config(\"spark.cores.max\", \"6\")\\\n",
    "            .config(\"spark.executor.memory\", \"3G\")\\\n",
    "            .config(\"spark.driver.memory\", \"2G\")\\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36056f-7fcb-4ce5-aeae-8dfbeff3c312",
   "metadata": {},
   "source": [
    "# Read Raw Car Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef24b61-d79b-4a99-9a54-ddce43bf447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data. We don't ask Spark to determine the column data types. \n",
    "# This can add time to the job. We can give it a schema in the read command or we can cast these once loaded. \n",
    "# Here, we'll cast the columns to the appropriate types once loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d9661-e9e8-4b01-9453-b33029f837c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n",
    "\n",
    "car_df = spark\\\n",
    "    .read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .option(\"inferSchema\", False)\\\n",
    "    .csv(\"/data/car_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1bc0b-823a-4a0d-bf28-ddcdbc9beb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of rows and columns in the dataframe\n",
    "\n",
    "print('Rows: {}, Columns: {}'.format(car_df.count(), len(car_df.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43765b96-9e67-4437-83c1-085b8861a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataframe schema.\n",
    "\n",
    "# Column descriptions are : \n",
    "# Ref : https://www.kaggle.com/aishu2218/do-you-wanna-predict-price-of-car-you-wanna-buy/data?select=car+data.csv\n",
    "\n",
    "\n",
    "# Car_Name :      The name of the car.\n",
    "# Year :          The year in which the car was bought.\n",
    "# Selling_Price : The price the owner wants to sell the car at.\n",
    "# Present_Price : The current ex-showroom price of the car.\n",
    "# Kms_Driven :    The distance completed by the car in km.\n",
    "# Fuel_Type :     Fuel type of the car.\n",
    "# Seller_Type :   Whether the seller is a dealer or an individual.\n",
    "# Transmission:   Whether the car is manual or automatic.\n",
    "# Owner:          The number of owners the car has previously had.\n",
    "\n",
    "car_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef9759-acb8-4239-b6ff-a61f6dedd4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few sample records. \n",
    "\n",
    "# Note that Prices are in lakh units. \n",
    "# https://en.wikipedia.org/wiki/Lakh ... In Indian numbering system equal to one hundred thousand. \n",
    "# For example, in India 150,000 rupees becomes 1.5 lakh rupees.\n",
    "\n",
    "car_df.show(10, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63ce6b-2a2d-4e1f-a44a-a1b1c3c97703",
   "metadata": {},
   "source": [
    "# Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577a714-4c85-4de7-8c0e-fee1c46a43ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDDs in Apache Spark are collection of partitions. Spark automatically partitions RDDs and distributes the partitions \n",
    "# across different nodes. A partition in spark is an atomic chunk of data (logical division of data) stored on a node in the cluster. \n",
    "# Partitions are basic units of parallelism in Apache Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea30059-e0ca-45f6-b408-d2f2b595c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When processing, Spark assigns one task for each partition and each worker threads can only process one task at a time. \n",
    "# Thus, with too few partitions, the application wonâ€™t utilize all the cores available in the cluster and it \n",
    "# can cause data skewing problem; with too many partitions, it will bring overhead for Spark to manage too \n",
    "# many small tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a84de-b444-4a5f-9699-1183ac820c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Spark, by default, a partition is created for every HDFS partition of size 64MB. \n",
    "# RDDs are automatically partitioned in spark without human intervention, however, at times the \n",
    "# programmers would like to change the partitioning scheme by changing the size of the partitions \n",
    "# and number of partitions based on the requirements of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f527f-7a9d-480e-86e6-1a1caebef7e3",
   "metadata": {},
   "source": [
    "![partitions](media/partitioning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225d919-1535-4fb0-95ca-b1b78cb48bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help with parallelism, shuffle data to where we have as many partitions as the number of available core\n",
    "# (e.g. 6 in this case because our cluster is configured with 6 cores).\n",
    "\n",
    "print('Number of partitions for car_df dataframe. Before repartition : {}'.format(car_df.rdd.getNumPartitions()))\n",
    "\n",
    "\n",
    "# Show the number of records per partition before the shuffle. Note that there is a cost to shuffling data.\n",
    "\n",
    "car_df\\\n",
    "    .withColumn(\"partitionId\", F.spark_partition_id())\\\n",
    "    .groupBy(\"partitionId\")\\\n",
    "    .count()\\\n",
    "    .orderBy(F.asc(\"count\"))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b1123-f4bf-4854-84c6-c9ee635ba23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition data.\n",
    "\n",
    "car_df = car_df.repartition(6) \n",
    "\n",
    "\n",
    "# Show the number of records per partition after the shuffle.\n",
    "\n",
    "print('Number of partitions for car_df dataframe. After repartition : {}'.format(car_df.rdd.getNumPartitions()))\n",
    "\n",
    "car_df\\\n",
    "    .withColumn(\"partitionId\", F.spark_partition_id())\\\n",
    "    .groupBy(\"partitionId\")\\\n",
    "    .count()\\\n",
    "    .orderBy(F.asc(\"partitionId\"))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae2635-1b68-4ac9-ba98-63a3dc79eabe",
   "metadata": {},
   "source": [
    "# Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04506a-6591-44fc-b03f-6a2fb4bb3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign proper types to certain column elements.\n",
    "\n",
    "car_df = car_df\\\n",
    "    .withColumn('Year', car_df.Year.cast(T.IntegerType()))\\\n",
    "    .withColumn('Kms_Driven', car_df.Kms_Driven.cast(T.IntegerType()))\\\n",
    "    .withColumn('Owner', car_df.Owner.cast(T.IntegerType()))\\\n",
    "    .withColumn('Selling_Price', car_df.Selling_Price.cast(T.DoubleType()))\\\n",
    "    .withColumn('Present_Price', car_df.Present_Price.cast(T.DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047da777-ece7-4a1e-949c-31d1575a44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema to see the changes\n",
    "\n",
    "car_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f896c9-cd94-407c-aff8-bbce8ffdc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure that we have data in every column of every row.\n",
    "# The query below counts all instances of null or nan in each colum of the car_df dataframe.\n",
    "\n",
    "car_df.select(\n",
    "    [F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in car_df.columns]\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01aaa3-ec68-4002-8c60-89ce5462c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to calculate the age of a vehicle given year\n",
    "\n",
    "def get_age(i_year):\n",
    "    current_year = date.today().year\n",
    "\n",
    "    if current_year < i_year:\n",
    "        raise Exception('Problem with years')\n",
    "\n",
    "    return current_year - i_year\n",
    "\n",
    "\n",
    "# Create a user defined function (i.e. a user-programmable routine that act on one row).\n",
    "\n",
    "calc_age_udf = F.udf(lambda year: get_age(year), T.IntegerType())\n",
    "\n",
    "\n",
    "\n",
    "# Add an Car_Age column to the car_df dataframe\n",
    "\n",
    "car_df = car_df\\\n",
    "    .withColumn('Car_Age', calc_age_udf(F.col('Year')))\n",
    "\n",
    "# Check our work\n",
    "\n",
    "car_df.select('Year', 'Car_Age').show(10, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eca127-0010-448a-aa24-03b8fd7c285f",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade8470-4388-4897-9ae3-acfe2b5cff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics for numeric columns.\n",
    "\n",
    "car_df\\\n",
    "    .select('Selling_Price', 'Present_Price', 'Kms_Driven', 'Car_Age')\\\n",
    "    .summary()\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaac0a0-3c32-4853-8b91-f985a61178b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Spark dataframe to a native Pandas dataframe (for visualizations). \n",
    "# Note that we are NOT using the NEW Pandas API on Spark, which allows \n",
    "# you to scale your Pandas workload out. It's just straight Pandas.\n",
    "\n",
    "car_pdf = car_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791c0c7-fe0d-4201-ad7a-7e572898d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for numerical data.\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 7))\n",
    "fig.suptitle('Numerical Data Box Plots')\n",
    "\n",
    "ax1 = sns.boxplot(x='Selling_Price', data=car_pdf, ax=ax1)\n",
    "ax2 = sns.boxplot(x='Kms_Driven', data=car_pdf, ax=ax2)\n",
    "ax3 = sns.boxplot(x='Present_Price', data=car_pdf, ax=ax3)\n",
    "ax4 = sns.boxplot(x='Car_Age', data=car_pdf, ax=ax4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e8293-9176-40a2-9c8b-4278c6d6098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar charts for categorical data.\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 11))\n",
    "fig.suptitle('Counts by Categories')\n",
    "\n",
    "ax1 = sns.countplot(x='Seller_Type', data=car_pdf, ax=ax1).set_title('Sale Count by Seller Type')\n",
    "ax2 = sns.countplot(x='Fuel_Type', data=car_pdf, ax=ax2).set_title('Sale Count by Fuel Type')\n",
    "ax3 = sns.countplot(x='Transmission', data=car_pdf, ax=ax3).set_title('Sale Count by Transmission')\n",
    "ax4 = sns.countplot(x='Owner', data=car_pdf, ax=ax4).set_title('Sale Count by Previous Owner Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d4109-ae58-4315-8e74-8c74afe5bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap.\n",
    "\n",
    "sns.heatmap(car_pdf.corr(), annot=True, cmap=\"RdBu\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4242e3-3139-4b8e-a5fb-0f5c1188b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot selling_price relative to present_price.\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.lmplot(x='Present_Price',y='Selling_Price',data=car_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06150352-c772-4db9-8fd3-8471853360ac",
   "metadata": {},
   "source": [
    "# Save the Spark Dataframe as a Parquet File for Downstream Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde1492-451d-4e6d-b9b6-4f20652c2150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema of the dataframe we are saving\n",
    "\n",
    "car_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c36e4-ae87-4500-80b6-7b441574ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.write.mode(\"overwrite\").parquet(\"/data/car_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9068b58-9195-48ef-9808-d13c5a061a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23c975-6b28-45b5-8035-31b4cad08c70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
