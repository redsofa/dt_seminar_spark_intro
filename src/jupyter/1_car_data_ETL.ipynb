{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2921fb-9991-4619-92d8-9fed700765c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This notebook uses Spark import and manipulate the car_data.csv data file. \n",
    "# It combines interesting sections from these 3 notebooks : \n",
    "#\n",
    "# https://www.kaggle.com/mohaiminul101/car-price-prediction\n",
    "# https://www.kaggle.com/aishu2218/do-you-wanna-predict-price-of-car-you-wanna-buy\n",
    "# https://www.kaggle.com/udit1907/linear-advanced-regression-guided-car-purchase\n",
    "\n",
    "# These notebooks use 'Pandas' and 'scikit-learn'. I primarily use SparkSQL and Spark MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628f022-841f-45ff-b183-ba865cafd2ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2924a6-78de-4c76-bf90-488331b27782",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<span style=\"color:blue\">\n",
    "    TODO : \n",
    "    <ul>\n",
    "      <li>Highlight pyspark.sql SparkSession import.<\\li>\n",
    "      <li>Spark canned set of functions and types.</li>\n",
    "      <li>Other imports are for date manipulations and plotting.</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b08a0-6afa-4e48-87a0-c6c34a39989b",
   "metadata": {},
   "source": [
    "# Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeac0e3-2cc2-4714-b986-8536a153c988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The simulated cluster environment is configured with : \n",
    "#   - 3 workers\n",
    "#   - With 3GB of memory for each worker (Total memory is 9GB)\n",
    "#   - Each worker has 2 cores (total cores is 6)\n",
    "\n",
    "# Start up Spark session. Let's be greedy and ask for all available resources (We'll be explicit).\n",
    "\n",
    "# Request : \n",
    "#   - A maximum of 6 cores with \n",
    "#   - 2 cores per executor\n",
    "#   - 3 GB of memory per executor\n",
    "\n",
    "# We also want to tell Spark about a specific java .jar file which contains a user defined function \n",
    "# we want to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab92e4-714a-4533-ba94-3a77690177af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .master(\"spark://spark-master:7077\")\\\n",
    "            .appName(\"1_car_data_ETL_jupyter\")\\\n",
    "            .config(\"spark.executor.cores\", \"2\")\\\n",
    "            .config(\"spark.cores.max\", \"6\")\\\n",
    "            .config(\"spark.executor.memory\", \"3G\")\\\n",
    "            .config(\"spark.driver.memory\", \"2G\")\\\n",
    "            .config('spark.jars', '/src/java/spark-jobs/helloworld/target/jv_helloworld-1.0-SNAPSHOT.jar')\\\n",
    "            .config('spark.executor.extraClassPath', '/src/java/spark-jobs/helloworld/target/jv_helloworld-1.0-SNAPSHOT.jar')\\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d7594-26b2-4304-addf-7d3ab554c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Spark session configuration. \n",
    "\n",
    "print(\"Spark Session configuration : \")\n",
    "\n",
    "print('===')\n",
    "\n",
    "for e in spark.sparkContext.getConf().getAll():\n",
    "    print(e)\n",
    "\n",
    "print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82dc76-95d8-4ddc-9f95-7a96d3b20fcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:blue\">\n",
    "    TODO : \n",
    "    <ul>\n",
    "      <li>Browse *http://spark-master:8080*. <\\li>\n",
    "      <li>Look at running applications names.</li>\n",
    "      <li>Look at worker core and memory usage.</li>\n",
    "      <li>Highlight external '.jar' usage in Session configuration.</li>\n",
    "      <li>Show StringLengthUDF class code in GitHub Project Repos.</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9162f04-322b-491e-a441-91788c032ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add jar to java spark context. It has a UDF that I want to use later.\n",
    "\n",
    "spark._jsc.addJar(\"/src/java/spark-jobs/helloworld/target/jv_helloworld-1.0-SNAPSHOT.jar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca9c2a-0a35-4902-a9fb-583b49f02706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the java function. It will be available as StringLengthUDF (in the spark.sql command).\n",
    "\n",
    "spark.udf.registerJavaFunction(\"StringLengthUDF\", \"ca.nrc.udf.StringLengthUDF\", T.IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee376ed-bc5e-4058-bd21-3d863fd332ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell Spark to use Apache Arrow\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b648c8-2782-4a52-a146-47cb0903ce17",
   "metadata": {},
   "source": [
    "# About the Data ... \n",
    "The **car_data.csv** file contains information about used cars. We'll use this data for the purposes of vehicle price prediction.\n",
    "\n",
    "### Columns and Descriptions\n",
    "* **Car_Name** :      The name of the car.\n",
    "* **Year** :          The year in which the car was bought.\n",
    "* **Selling_Price** : The price the owner wants to sell the car at.\n",
    "* **Present_Price** : The current ex-showroom price of the car.\n",
    "* **Kms_Driven** :    The distance completed by the car in km.\n",
    "* **Fuel_Type** :     Fuel type of the car.\n",
    "* **Seller_Type** :   Whether the seller is a dealer or an individual.\n",
    "* **Transmission** :   Whether the car is manual or automatic.\n",
    "* **Owner** :          The number of owners the car has previously had."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36056f-7fcb-4ce5-aeae-8dfbeff3c312",
   "metadata": {},
   "source": [
    "# Read Raw Car Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef24b61-d79b-4a99-9a54-ddce43bf447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data. We don't ask Spark to determine the column data types. \n",
    "# This can add time to the job. We can give it a schema in the read command \n",
    "# or we can cast these once loaded. \n",
    "\n",
    "# Here, we'll cast the columns to the appropriate types once loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d9661-e9e8-4b01-9453-b33029f837c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n",
    "\n",
    "car_df = spark\\\n",
    "    .read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .option(\"inferSchema\", False)\\\n",
    "    .csv(\"/data/car_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1bc0b-823a-4a0d-bf28-ddcdbc9beb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of rows and columns in the dataframe. \n",
    "# Similar to Pandas \"pandas.DataFrame.shape\" ...\n",
    "\n",
    "print('Rows: {}, Columns: {}'.format(car_df.count(), len(car_df.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43765b96-9e67-4437-83c1-085b8861a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataframe schema.\n",
    "# Similar to Pandas \"pandas.DataFrame.dtypes\" ...\n",
    "\n",
    "car_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef9759-acb8-4239-b6ff-a61f6dedd4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few sample records. \n",
    "\n",
    "# Note that Prices are in lakh units. \n",
    "# https://en.wikipedia.org/wiki/Lakh ... \n",
    "\n",
    "# A lakh, in Indian numbering system, is equal to one hundred thousand. \n",
    "# For example, in India 150,000 rupees becomes 1.5 lakh rupees.\n",
    "\n",
    "car_df.show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63ce6b-2a2d-4e1f-a44a-a1b1c3c97703",
   "metadata": {},
   "source": [
    "# Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577a714-4c85-4de7-8c0e-fee1c46a43ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitions are basic units of parallelism in Apache Spark. \n",
    "# With too few partitions, the application wonâ€™t utilize all the cores available in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea30059-e0ca-45f6-b408-d2f2b595c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, Spark creates partitions that are equal to the number of CPU cores in the machine (spark.default.parallelism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a84de-b444-4a5f-9699-1183ac820c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When Spark reads a CSV file, it splits up the data into multiple partitions based on \n",
    "# the configuration \"spark.files.maxPartitionBytes\" which defaults to 128MB.\n",
    "\n",
    "# RDDs are automatically partitioned in spark without human intervention. \n",
    "# However, we can change the partitioning scheme if we want. \n",
    "# Note that there is a cost to shuffling data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f527f-7a9d-480e-86e6-1a1caebef7e3",
   "metadata": {},
   "source": [
    "![partitions](media/partitioning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225d919-1535-4fb0-95ca-b1b78cb48bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the current partitioning scheme ? \n",
    "\n",
    "print('Number of partitions for car_df dataframe. Before repartition : {}'.format(car_df.rdd.getNumPartitions()))\n",
    "\n",
    "\n",
    "# Show the number of records per partition before the shuffle. \n",
    "\n",
    "car_df\\\n",
    "    .withColumn(\"partitionId\", F.spark_partition_id())\\\n",
    "    .groupBy(\"partitionId\")\\\n",
    "    .count()\\\n",
    "    .orderBy(F.asc(\"count\"))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b1123-f4bf-4854-84c6-c9ee635ba23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Repartition data for fun. Shuffle data to where we have as many partitions as the number of available core\n",
    "# (e.g. 6 in this case because our cluster is configured with 6 cores).\n",
    "\n",
    "car_df = car_df.repartition(6) \n",
    "\n",
    "\n",
    "# Show the number of records per partition after the shuffle.\n",
    "\n",
    "print('Number of partitions for car_df dataframe. After repartition : {}'.format(car_df.rdd.getNumPartitions()))\n",
    "\n",
    "car_df\\\n",
    "    .withColumn(\"partitionId\", F.spark_partition_id())\\\n",
    "    .groupBy(\"partitionId\")\\\n",
    "    .count()\\\n",
    "    .orderBy(F.asc(\"partitionId\"))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0227bb-8826-45eb-940d-28ebb8259efe",
   "metadata": {},
   "source": [
    "# DataFrames API - Selection, Aggregation, Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748e7fb-6135-4da1-960c-1b8668b30809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do any number of transformations using the DataFrames API.\n",
    "# This is to give you an idea of what interacting with dataframes looks like.\n",
    "\n",
    "# What kind of fuels the vehicles run on (Selection, Aggregation) ?\n",
    "\n",
    "fuels_df = car_df\\\n",
    "    .select('Fuel_Type')\\\n",
    "    .groupBy('Fuel_Type')\\\n",
    "    .agg(F.count(\"Fuel_Type\").alias('number_of_vehicles'))\\\n",
    "    .orderBy(F.col('number_of_vehicles').desc())\n",
    "\n",
    "fuels_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7b60e-f74f-4a42-a634-a7f766534131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same with a temporary table and a SQL query:\n",
    "\n",
    "# Register the car_df DataFrame as a SQL temporary view\n",
    "car_df.createOrReplaceTempView(\"CAR_TABLE\")\n",
    "\n",
    "\n",
    "# What kind of fuels the vehicles run on (Selection, Aggregation) ?\n",
    "\n",
    "# Create the fuels_df DataFrame using Standard SQL query\n",
    "fuels_df = spark.sql('''\n",
    "    SELECT\n",
    "        Fuel_Type,\n",
    "        COUNT(1) AS number_of_vehicles\n",
    "    FROM \n",
    "        CAR_TABLE\n",
    "    GROUP BY\n",
    "        Fuel_Type\n",
    "    ORDER BY\n",
    "        number_of_vehicles\n",
    "    DESC\n",
    "''')\n",
    "\n",
    "fuels_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3010040-cf48-4d2c-b8c9-3c13c16fb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering example :\n",
    "\n",
    "# Cars selling for over 30 lakh rupees. \n",
    "\n",
    "car_df\\\n",
    "    .filter(F.col('Selling_Price').cast(T.DoubleType()) >= '30.0')\\\n",
    "    .show()\n",
    "\n",
    "# Note that we have to cast the Selling_Price as a double. It's still a string at this point.\n",
    "# We will fix this permanently in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34370e1-fb28-46d8-9ef0-b9636d83b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A point for the section above is this : \n",
    "\n",
    "\n",
    "# PySpark APIs for working with data are quite easy to use and maybe even easier to work with than other APIs.. (Looking at you Pandas).\n",
    "\n",
    "\n",
    "# !! HOWEVER !! \n",
    "\n",
    "# If you still want to something like a Pandas API but benefit from Spark's distributed architecture, \n",
    "# check out (Pandas API on Spark - used to be Koalas). This is new in Spark 3.2.\n",
    "\n",
    "# https://databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83fcc6-42dd-4858-a41a-1a9b9d120609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More transformation examples can be found in the common_ops jupyter notebook included in the current GitHub project directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae2635-1b68-4ac9-ba98-63a3dc79eabe",
   "metadata": {},
   "source": [
    "# Data Transformations for Downstream MLlib Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04506a-6591-44fc-b03f-6a2fb4bb3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign proper types to certain column elements.\n",
    "\n",
    "car_df = car_df\\\n",
    "    .withColumn('Year', car_df.Year.cast(T.IntegerType()))\\\n",
    "    .withColumn('Kms_Driven', car_df.Kms_Driven.cast(T.IntegerType()))\\\n",
    "    .withColumn('Owner', car_df.Owner.cast(T.IntegerType()))\\\n",
    "    .withColumn('Selling_Price', car_df.Selling_Price.cast(T.DoubleType()))\\\n",
    "    .withColumn('Present_Price', car_df.Present_Price.cast(T.DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047da777-ece7-4a1e-949c-31d1575a44ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the schema to see the changes\n",
    "\n",
    "car_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a4bfa-2b92-4fb0-acba-6d7f723f8be5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:blue\">\n",
    "    TODO : \n",
    "    <ul>\n",
    "      <li>Highlight type changes in table schema.</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f896c9-cd94-407c-aff8-bbce8ffdc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure that we have data in every column of every row.\n",
    "# The query below counts all instances of null or nan in each colum of the car_df dataframe.\n",
    "\n",
    "car_df.select(\n",
    "    [F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in car_df.columns]\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01aaa3-ec68-4002-8c60-89ce5462c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and call a User Defined Function (UDF) written in Python \n",
    "\n",
    "\n",
    "# Write a function to calculate the age of a vehicle given year\n",
    "def get_age(i_year):\n",
    "    current_year = date.today().year\n",
    "\n",
    "    if current_year < i_year:\n",
    "        raise Exception('Problem with years')\n",
    "\n",
    "    return current_year - i_year\n",
    "\n",
    "\n",
    "# Create a user defined function (i.e. a user-programmable routine that act on one row).\n",
    "\n",
    "calc_age_udf = F.udf(lambda year: get_age(year), T.IntegerType())\n",
    "\n",
    "\n",
    "# Add an 'Car_Age' column to the car_df dataframe\n",
    "\n",
    "car_df = car_df\\\n",
    "    .withColumn('Car_Age', calc_age_udf(F.col('Year')))\n",
    "\n",
    "# We should see a 'Car_Age' column in at the end of our dataframe now.\n",
    "\n",
    "car_df.show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba0c39-aa7e-4c8e-b033-7892b7262422",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:blue\">\n",
    "    TODO : \n",
    "    <ul>\n",
    "      <li>Mention get_age() function could be part of larger shared/reusable library with unit testing and \"all the trimmings\".</li>  \n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b4f62-4d12-43ff-8a2d-f14d4164554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call a UDF written in Java\n",
    "\n",
    "# Add the first name length field (fname_length) by calling a Java function. \n",
    "# This function lives in the jar we added to Spark earlier.\n",
    "car_df = car_df\\\n",
    "    .withColumn(\"car_name_length\", F.expr(\"StringLengthUDF(Car_Name)\"))\n",
    "\n",
    "car_df.show(5, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3ce98-2f26-4de6-8857-32ee22c483cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also call the Java UDF using a SQL query. \n",
    "# Let's do that but just show what the transformation would look like.\n",
    "\n",
    "# Re-register the car_df DataFrame as a SQL temporary view. \n",
    "# To make sure we update the table with new type information.\n",
    "car_df.createOrReplaceTempView(\"CAR_TABLE\")\n",
    "\n",
    "spark.sql('''\n",
    "    SELECT\n",
    "        Car_Name,\n",
    "        Year,\n",
    "        Selling_Price,\n",
    "        Present_Price,\n",
    "        Kms_Driven,\n",
    "        Fuel_Type,\n",
    "        Seller_Type,\n",
    "        Transmission,\n",
    "        Owner,\n",
    "        Car_Age,\n",
    "        StringLengthUDF(Car_Name) as car_name_length\n",
    "    FROM \n",
    "        CAR_TABLE\n",
    "''').show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a060a-401a-4aca-9b00-ed6531fd44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One last simple tranformation example :\n",
    "\n",
    "# Add calculated column. Create 'inflated_price' column, which is the ('Selling_Price' + 30 percent) :\n",
    "\n",
    "car_df = car_df\\\n",
    "    .withColumn('inflated_price', F.col('Selling_Price')*F.lit(1.30))\n",
    "\n",
    "car_df\\\n",
    "    .select('Car_Name', 'Selling_Price', 'Inflated_Price')\\\n",
    "    .show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f124e-8ddd-4515-a038-d9ab635abdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a column we don't need .. in this case, the 'Inflated_Price' and 'car_name_length' columns we just added.\n",
    "\n",
    "car_df = car_df\\\n",
    "    .drop('Inflated_Price')\\\n",
    "    .drop('car_name_length')\n",
    "\n",
    "car_df.show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eca127-0010-448a-aa24-03b8fd7c285f",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade8470-4388-4897-9ae3-acfe2b5cff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics for numeric columns.\n",
    "\n",
    "car_df\\\n",
    "    .select('Selling_Price', 'Present_Price', 'Kms_Driven', 'Car_Age')\\\n",
    "    .summary()\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaac0a0-3c32-4853-8b91-f985a61178b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Spark dataframe to a native Pandas dataframe (for visualizations). \n",
    "# Note that we are NOT using the NEW Pandas API on Spark, which allows \n",
    "# you to scale your Pandas workload out. It's just straight Pandas.\n",
    "\n",
    "car_pdf = car_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb24a6-c601-4e0f-a762-8a5a2c9e55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SIDE NOTE ... APACHE ARROW\n",
    "\n",
    "# If using Apacke Arrow, creating a Spark dataframe from a pandas dataframe is much quicker.\n",
    "# We are already using apache arrow.. This line was executed above.\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# How long does it take to create a Spark dataframe from a pandas dataframe if using Arrow?\n",
    "%time df = spark.createDataFrame(car_pdf)\n",
    "print()\n",
    "print()\n",
    "# If using NOT using Apacke Arrow, creating a Spark dataframe from a pandas dataframe should be slower.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "%time df = spark.createDataFrame(car_pdf)\n",
    "\n",
    "# Reset to use Apache Arrow\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c64e7-3796-4f63-bac1-2abfe624c46f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:blue\">\n",
    "    TODO : \n",
    "    <ul>\n",
    "      <li>Highlight dataframe creation speed difference (with and without arrow)</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791c0c7-fe0d-4201-ad7a-7e572898d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for numerical data.\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 7))\n",
    "fig.suptitle('Numerical Data Box Plots')\n",
    "\n",
    "ax1 = sns.boxplot(x='Selling_Price', data=car_pdf, ax=ax1)\n",
    "ax2 = sns.boxplot(x='Kms_Driven', data=car_pdf, ax=ax2)\n",
    "ax3 = sns.boxplot(x='Present_Price', data=car_pdf, ax=ax3)\n",
    "ax4 = sns.boxplot(x='Car_Age', data=car_pdf, ax=ax4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e8293-9176-40a2-9c8b-4278c6d6098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar charts for categorical data.\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 11))\n",
    "fig.suptitle('Counts by Categories')\n",
    "\n",
    "ax1 = sns.countplot(x='Seller_Type', data=car_pdf, ax=ax1).set_title('Sale Count by Seller Type')\n",
    "ax2 = sns.countplot(x='Fuel_Type', data=car_pdf, ax=ax2).set_title('Sale Count by Fuel Type')\n",
    "ax3 = sns.countplot(x='Transmission', data=car_pdf, ax=ax3).set_title('Sale Count by Transmission')\n",
    "ax4 = sns.countplot(x='Owner', data=car_pdf, ax=ax4).set_title('Sale Count by Previous Owner Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d4109-ae58-4315-8e74-8c74afe5bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap.\n",
    "\n",
    "sns.heatmap(car_pdf.corr(), annot=True, cmap=\"RdBu\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4242e3-3139-4b8e-a5fb-0f5c1188b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot selling_price relative to present_price.\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.lmplot(x='Present_Price',y='Selling_Price',data=car_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06150352-c772-4db9-8fd3-8471853360ac",
   "metadata": {},
   "source": [
    "# Save the Spark Dataframe as a Parquet File for Downstream Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde1492-451d-4e6d-b9b6-4f20652c2150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema of the dataframe we are saving\n",
    "\n",
    "car_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c36e4-ae87-4500-80b6-7b441574ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.write.mode(\"overwrite\").parquet(\"/data/car_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bce8f0-d859-4c7f-afee-dfd8a4f66b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo 'Current Linux User : (Should be ds...)'\n",
    "whoami\n",
    "\n",
    "echo ''\n",
    "echo ''\n",
    "\n",
    "\n",
    "# We can see the parquet file (6 partitions) was saved in the '/data' directory : \n",
    "\n",
    "echo 'List parquet file parts :'\n",
    "\n",
    "ls /data/car_data.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9068b58-9195-48ef-9808-d13c5a061a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4f4e3-7af1-45c6-b863-020e1b30d63b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
